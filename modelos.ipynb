{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c02b6d0",
   "metadata": {},
   "source": [
    "# Modelos CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d95b581",
   "metadata": {},
   "source": [
    "# Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425e3697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90931cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation utilities\n",
    "from torchvision.transforms import functional as F\n",
    "import random\n",
    "\n",
    "class AdvancedAugmentation:\n",
    "    \"\"\"Custom data augmentation for our drawing dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        # Apply transformations with probability\n",
    "        if random.random() < self.prob:\n",
    "            # Random rotation (± 20 degrees)\n",
    "            angle = random.uniform(-20, 20)\n",
    "            img = F.rotate(img, angle, fill=0)\n",
    "        \n",
    "        if random.random() < self.prob:\n",
    "            # Random shifts (± 15%)\n",
    "            shift_x = random.uniform(-0.15, 0.15)\n",
    "            shift_y = random.uniform(-0.15, 0.15)\n",
    "            img = F.affine(img, angle=0, translate=[shift_x, shift_y], \n",
    "                           scale=1.0, shear=0, fill=0)\n",
    "        \n",
    "        if random.random() < 0.3:\n",
    "            # Random erasing (simulates occlusions)\n",
    "            h, w = img.shape[1:]\n",
    "            area = h * w\n",
    "            target_area = random.uniform(0.02, 0.15) * area\n",
    "            aspect_ratio = random.uniform(0.3, 1/0.3)\n",
    "            \n",
    "            h_rect = int(np.sqrt(target_area * aspect_ratio))\n",
    "            w_rect = int(np.sqrt(target_area / aspect_ratio))\n",
    "            \n",
    "            if h_rect < h and w_rect < w:\n",
    "                x1 = random.randint(0, h - h_rect)\n",
    "                y1 = random.randint(0, w - w_rect)\n",
    "                mask = torch.ones_like(img)\n",
    "                mask[:, x1:x1+h_rect, y1:y1+w_rect] = 0\n",
    "                img = img * mask\n",
    "        \n",
    "        if random.random() < 0.2:\n",
    "            # Add slight Gaussian noise\n",
    "            noise = torch.randn_like(img) * 0.05\n",
    "            img = torch.clamp(img + noise, 0, 1)\n",
    "            \n",
    "        if random.random() < 0.2:\n",
    "            # Adjust brightness/contrast\n",
    "            brightness = random.uniform(-0.2, 0.2)\n",
    "            contrast = random.uniform(0.8, 1.2)\n",
    "            img = F.adjust_brightness(img, 1 + brightness)\n",
    "            img = F.adjust_contrast(img, contrast)\n",
    "        \n",
    "        return img\n",
    "\n",
    "# Visualize augmentations\n",
    "def visualize_augmentations(dataset, num_images=5, num_aug=3):\n",
    "    \"\"\"Visualize original and augmented versions of images from the dataset\"\"\"\n",
    "    augmenter = AdvancedAugmentation(prob=0.8)\n",
    "    \n",
    "    plt.figure(figsize=(num_aug+1, num_images*2))\n",
    "    for i in range(num_images):\n",
    "        img, label = dataset[i]\n",
    "        \n",
    "        # Show original\n",
    "        plt.subplot(num_images, num_aug+1, i*(num_aug+1)+1)\n",
    "        plt.imshow(img.squeeze(), cmap='gray')\n",
    "        plt.title(f\"Original: {dataset.classes[label]}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Show augmentations\n",
    "        for j in range(num_aug):\n",
    "            aug_img = augmenter(img.clone())\n",
    "            plt.subplot(num_images, num_aug+1, i*(num_aug+1)+j+2)\n",
    "            plt.imshow(aug_img.squeeze(), cmap='gray')\n",
    "            plt.title(f\"Aug {j+1}\")\n",
    "            plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003abb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "start_time = time.time()\n",
    "\n",
    "# ---------- Dataset ----------\n",
    "# Basic transformation for validation and testing\n",
    "basic_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1), \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Advanced transformation with augmentation for training\n",
    "augmentation_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    AdvancedAugmentation(prob=0.7)\n",
    "])\n",
    "\n",
    "data_dir = \"data/train\"\n",
    "# Use augmented transformations for training\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=augmentation_transform)\n",
    "\n",
    "class_names = dataset.classes\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# ---------- Validació ----------\n",
    "val_dir = \"data/validation\"\n",
    "# Use basic transform without augmentation for validation\n",
    "val_dataset = datasets.ImageFolder(root=val_dir, transform=basic_transform)\n",
    "\n",
    "# Visualize some augmented samples\n",
    "visualize_augmentations(datasets.ImageFolder(root=data_dir, transform=basic_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a9451207",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "max_total_time = 1800 \n",
    "\n",
    "num_core = multiprocessing.cpu_count()\n",
    "\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=True, \n",
    "                        num_workers=num_core-1,\n",
    "                        persistent_workers=True,\n",
    "                        prefetch_factor=4)\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=False, \n",
    "                        num_workers=num_core-1,\n",
    "                        persistent_workers=True,\n",
    "                        prefetch_factor=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641404d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# ---------- Model ----------\\nclass ElVostreModel(nn.Module):\\n    def __init__(self, num_classes):\\n        super(ElVostreModel, self).__init__()\\n        self.net = nn.Sequential(\\n            nn.Flatten(),\\n            nn.Linear(28*28, 256),\\n            nn.ReLU(),\\n            nn.Linear(256, num_classes)\\n        )\\n\\n    def forward(self, x):\\n        return self.net(x)\\n\\n'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImprovedCNNModel(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.25, fc_dropout=0.5):\n",
    "        super(ImprovedCNNModel, self).__init__()\n",
    "        \n",
    "        # First convolutional block: 2x Conv2D + MaxPool + Dropout\n",
    "        self.conv1_1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1_1 = nn.BatchNorm2d(32)\n",
    "        self.conv1_2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn1_2 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout2d(dropout_rate)\n",
    "        \n",
    "        # Second convolutional block: 2x Conv2D + MaxPool + Dropout\n",
    "        self.conv2_1 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2_1 = nn.BatchNorm2d(64)\n",
    "        self.conv2_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn2_2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout2 = nn.Dropout2d(dropout_rate)\n",
    "        \n",
    "        # Third convolutional block: Conv2D + Global Average Pooling\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(128, 512)\n",
    "        self.bn_fc = nn.BatchNorm1d(512)\n",
    "        self.fc_dropout = nn.Dropout(fc_dropout)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = F.relu(self.bn1_1(self.conv1_1(x)))\n",
    "        x = F.relu(self.bn1_2(self.conv1_2(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Second block\n",
    "        x = F.relu(self.bn2_1(self.conv2_1(x)))\n",
    "        x = F.relu(self.bn2_2(self.conv2_2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Third block\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.global_pool(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.bn_fc(self.fc1(x)))\n",
    "        x = self.fc_dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Original model (for reference)\n",
    "class ElVostreModel(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.3):\n",
    "        super(ElVostreModel, self).__init__()\n",
    "        \n",
    "        # Convolutional layers with batch normalization\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Pooling and dropout\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.dropout2d = nn.Dropout2d(0.2)\n",
    "        \n",
    "        # Adaptive pooling for flexibility\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First conv block\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.dropout2d(x)\n",
    "        \n",
    "        # Second conv block\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.dropout2d(x)\n",
    "        \n",
    "        # Third conv block\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.adaptive_pool(x)\n",
    "        \n",
    "        # Flatten and fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "'''\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class AdvancedCNNModel(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.3):\n",
    "        super(AdvancedCNNModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.layer1 = ResidualBlock(32, 64, stride=2)\n",
    "        self.layer2 = ResidualBlock(64, 128, stride=2)\n",
    "        \n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bef1bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "# Use the improved model based on the recommended structure\n",
    "model = ImprovedCNNModel(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3e7fe1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dc15691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, name):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f\"{name} Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4cf4fa1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validació (després de la època 1[168.87s]) Accuracy: 45.65%\n",
      "Nou millor model guardat amb 45.65% de precisió de validació\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validació (després de la època 2[129.92s]) Accuracy: 58.70%\n",
      "Nou millor model guardat amb 58.70% de precisió de validació\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validació (després de la època 3[128.88s]) Accuracy: 59.42%\n",
      "Nou millor model guardat amb 59.42% de precisió de validació\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validació (després de la època 4[130.30s]) Accuracy: 68.12%\n",
      "Nou millor model guardat amb 68.12% de precisió de validació\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validació (després de la època 5[130.54s]) Accuracy: 65.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validació (després de la època 6[129.11s]) Accuracy: 67.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validació (després de la època 7[129.44s]) Accuracy: 68.84%\n",
      "Nou millor model guardat amb 68.84% de precisió de validació\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validació (després de la època 8[129.24s]) Accuracy: 68.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validació (després de la època 9[128.54s]) Accuracy: 68.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validació (després de la època 10[129.66s]) Accuracy: 66.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validació (després de la època 11[129.47s]) Accuracy: 70.29%\n",
      "Nou millor model guardat amb 70.29% de precisió de validació\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validació (després de la època 12[128.74s]) Accuracy: 68.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validació (després de la època 13[129.55s]) Accuracy: 68.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps màxim assolit. Fi de l'entrenament\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "\n",
    "best_val_acc = 0.0\n",
    "model_save_path = \"best_model.pth\"\n",
    "\n",
    "\n",
    "while True:\n",
    "    start_epoch = time.time()\n",
    "    epoch += 1\n",
    "    model.train()\n",
    "    loop = tqdm(dataloader, desc=f\"Època {epoch}\", leave=False)\n",
    "\n",
    "    for i, (images, labels) in enumerate(loop):\n",
    "        if time.time() - start_time > max_total_time:\n",
    "            print(\"Temps màxim assolit. Fi de l'entrenament\")\n",
    "            break\n",
    "    \n",
    "\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        # ...\n",
    "    \n",
    "    if time.time() - start_time > max_total_time:\n",
    "        break\n",
    "    # Validació per època\n",
    "    val_acc = evaluate(model, val_loader, f\"Validació (després de la època {epoch}[{time.time() - start_epoch:.2f}s])\")\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Nou millor model guardat amb {best_val_acc * 100:.2f}% de precisió de validació\")\n",
    "    # Validació per època"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a450ec",
   "metadata": {},
   "source": [
    "# Final Avaluacio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625949d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_model(path, num_classes):\n",
    "    \"\"\"Carga el mejor modelo guardado.\"\"\"\n",
    "    # Update to use the improved model\n",
    "    model = ImprovedCNNModel(num_classes)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eccfa8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (subset) Accuracy: 85.81%\n",
      "Validation (final) Accuracy: 70.29%\n",
      "\n",
      "Final Metrics:\n",
      "   Train Accuracy: 85.81%\n",
      "   Validation Accuracy: 70.29%\n",
      "Temps total d'entrenament: 1846.96 segons\n"
     ]
    }
   ],
   "source": [
    "best_model = load_best_model(model_save_path, num_classes)\n",
    "# Evaluar el modelo en el conjunto de entrenamiento y validación\n",
    "train_acc = evaluate(best_model,dataloader, \"Train (subset)\")\n",
    "val_acc = evaluate(best_model, val_loader, \"Validation (final)\")\n",
    "\n",
    "print(f\"\\nFinal Metrics:\")\n",
    "print(f\"   Train Accuracy: {train_acc * 100:.2f}%\")\n",
    "print(f\"   Validation Accuracy: {val_acc * 100:.2f}%\")\n",
    "print(f\"Temps total d'entrenament: {time.time() - start_time:.2f} segons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c388dd",
   "metadata": {},
   "source": [
    "## Model Architecture Analysis\n",
    "\n",
    "The model architecture implemented follows the recommended structure:\n",
    "\n",
    "1. First convolutional block:\n",
    "   - Two Conv2D layers with 32 filters (3x3) + BatchNorm + ReLU\n",
    "   - MaxPool2D(2x2) + Dropout(0.25)\n",
    "\n",
    "2. Second convolutional block:\n",
    "   - Two Conv2D layers with 64 filters (3x3) + BatchNorm + ReLU\n",
    "   - MaxPool2D(2x2) + Dropout(0.25)\n",
    "\n",
    "3. Third convolutional block:\n",
    "   - One Conv2D layer with 128 filters (3x3) + BatchNorm + ReLU\n",
    "   - GlobalAveragePooling2D\n",
    "\n",
    "4. Fully connected layers:\n",
    "   - Dense(512) + BatchNorm + ReLU + Dropout(0.5)\n",
    "   - Dense(num_classes)\n",
    "\n",
    "This architecture is effective for our drawing classification task because:\n",
    "1. Multiple convolutional layers extract hierarchical features\n",
    "2. Batch normalization improves training stability and speed\n",
    "3. Dropout layers reduce overfitting\n",
    "4. Global average pooling reduces parameters and helps with spatial invariance\n",
    "5. The final dense layers learn high-level representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bf40a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Instalar scikit-learn si no está disponible\\nimport sys\\nimport subprocess\\n\\ndef install_package(package):\\n    print(f\"Instalando {package}...\")\\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\\n    print(f\"{package} instalado correctamente\")\\n\\ntry:\\n    # Intentar importar sklearn\\n    import sklearn\\n    print(f\"scikit-learn ya está instalado (versión {sklearn.__version__})\")\\nexcept ImportError:\\n    # Si falla, instalarlo\\n    install_package(\"scikit-learn\")\\n    print(\"Por favor, reinicia el kernel después de la instalación para usar sklearn\")'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
